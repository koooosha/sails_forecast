## Report
### Since there is no documentation available about the dataset and its features, I have decided to invest some time in thoroughly investigating the dataset. By carefully examining the data and analyzing its various features, I aim to gain insights that will help me better understand the problem and devise effective solutions.
Data analysis and forecasting consists of different steps from reading data to forecast result for unseen data.
1. Data Set Understanding
2. Problem Definition
3. Data Analysis
   1. EDA
   2. Data preprocessing and feature engineering
4. Modeling & Evaluation
   1. Traditional Model
   2. Machine Learning Model
   3. Deep Learning Model
5. Forecasting

## Data set Understanding:
A visual inspection of the dataset suggests that it may represent sales data from a store, with the variable "vkp_mge" indicating the quantity of sales. Among the features, those labeled from "a" to "i" appear to be binary variables, possibly indicating various flags related to dates, weather, or store conditions. Additionally, features such as "inout_IO", "inout_SO", "ausw_cd_A", "ausw_cd_W", and "aus_cd_G" seem to be categorical features that have been transformed into binary representations using one-hot encoding.

Upon closer examination, it becomes apparent that the "inout" variable has a constant value across all samples, providing no useful information for modeling purposes and thus has been removed. Similarly, the variable "son_vkp", which appears to represent a special sale price (always 0), has also been excluded.

Among the remaining features, those ending in "_vkp" likely represent sale prices, with "son_vkp" indicating a special sale price. The features "current price" and "system price" are represented as float values less than 1 by 4 digits, suggesting normalization or possibly representing a fraction or ratio. Notably, the system price consistently exceeds the other price.

Another significant feature, "am", is a float number with three distinct values in the dataset, indicating a categorical variable with discrete values. Similarly, "It" represents whether the store is open ("A") or closed ("N"). Instances where "It" is "N" correspond to days with zero sales, often Sundays and holidays. Additionally, there are periods marked as "N" which likely signify store closures for various reasons.

A challenging aspect of the dataset is a prolonged period where sales are consistently zero, spanning from "24-10-2018" to "04-03-2019". Despite the lack of information about this period, it's important not to disregard it outright as it may hold valuable insights and I prefer to keep this data. This period could reflect production downtime or operational changes preventing product sales. Another option could be replacing 0 by another value like the average or median of the Sales in the recent months.

In summary, the dataset appears to be a multivariate time series, comprising two time series ("vkp_mge" and "a") along with categorical features (binary and discrete values) acting as exogenous variables. These exogenous variables provide additional independent information, suggesting that the problem at hand involves multivariate time series forecasting with exogenous variables.

## Problem Definition
The second step involves defining the problem. During this phase, it's crucial to articulate the problem clearly, identify challenges, and outline the subsequent steps. The objective here is to forecast sales by developing a model using the provided features. Upon understanding the dataset, we observe that it can be viewed as a multivariate time series dataset, comprising 2 time series and multiple exogenous variables, including categorical or binary variables.

The first task is to conduct exploratory data analysis (EDA) on the dataset. This involves visualizing the data, checking for missing values or outliers, and exploring relationships between features, particularly "a" and the target time series, "vpk_mge". Additionally, we need to assess the stationarity of each time series and explore any seasonality present, which can aid in identifying patterns.

Following EDA, we may need to preprocess the data based on the chosen modeling technique for forecasting. This could involve converting non-stationary data to stationary, normalization, or standardization for machine learning or deep learning methods. Categorical features may need to be converted to binary representation using one-hot encoding, and feature selection may be necessary to remove redundant features.

Next, we must partition the dataset into training and evaluation subsets. For time series data, an 80% training and 20% evaluation split is commonly used, with the evaluation period representing the remaining time. To evaluate the trained model, we need to define appropriate metrics. For sales forecasting, metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) are commonly employed.

## EDA and Feature Engineering:
Some features have constant values or lack any values, making them ineffective for modeling and forecasting future values. Thus, it's prudent to remove them from our dataset. Below, I outline the challenges encountered during the exploratory data analysis (EDA) and feature engineering. Visualization results and scripts can be found in the Jupyter notebook.

Challenges:

1. The primary challenge encountered during analysis pertains to handling missing values. Fortunately, "vkp_mge" and "a" do not have any missing values, except for a period of approximately four months where "vkp_mge" remained constantly at 0. However, for binary and categorical values (exogenous variables), approximately 44% of the training dataset is missing. Simply removing them is not advisable, so alternative treatments need consideration.
An intriguing observation is that these missing values occur simultaneously, suggesting they may stem from a common source that was unavailable for some reason. When imputing missing values in time series data, various options are available, including filling them with 0 or default values (such as forward filling or backward filling), using statistical measures like mean and median, employing more advanced methods like interpolation, or even applying machine learning techniques. However, it's important to note that each of these methods requires domain knowledge and understanding of the variables to ensure accurate imputation. Given the lack of information about the features, simply imputing them with a value may not be the most appropriate solution for me.
To address this, I propose using a technique called "missing indicator imputation." Here, I'll define a new binary feature indicating whether these exogenous variables are missing. If this binary feature is set to 1, it indicates missing data, and NaN values can be replaced with 0. Conversely, if the binary flag is 0, it signifies that these features have a value. Additionally there exists another challenge for the "akt_vkp" and I can observe whenever the "am" feature is "0.54", "akt_vkp" is Nan so the pattern of missing values for this feature is a bit different from other features. So I defined a separate flag for this feature as an indicator of missing data replaced this values by "0". 

2. Another feature transformation involves the "It" feature, indicating whether the store was open or closed. To address this, I employed one-hot encoding techniques to transform this feature into two binary flags for "A" and "N".

During my exploratory data analysis (EDA) on the training dataset, I visualized various features both in their raw form and after grouping them by months or quarters, enabling the identification of different sales patterns. Another crucial analysis involved checking the stationarity of the time series, which is essential for model selection. I utilized the Augmented Dickey-Fuller (ADF) test, and the results indicated that both time series are stationary.

Further, I decomposed the sales time series into trend, seasonality, and residuals components to uncover any seasonal patterns in the data. Notably, some seasonality patterns were observed with a periodicity of 7 days, which must be taken into account during the modeling phase. The dataset spans approximately two years, with a prolonged period during fall and winter where sales were consistently zero. It's worth noting that these periods of zero sales, which may occur irregularly, could potentially influence statistical results at monthly or quarterly intervals. Collecting data over a longer period could provide insights into addressing these periods effectively.

A crucial aspect of multivariate time series analysis involves assessing whether the features and the target variable exhibit causal relationships. The Granger causality test is an effective method for checking causality, and the resulting p-value indicates that "vkmg" is influenced by "a". It's essential to examine correlation relationships among features in most data science projects. The correlation analysis between "vkmge" and other features revealed no significant correlation. This was expected, as visually observed instances showed that while the value of "a" increased in some cases, the value of "vkmge" increased in others and decreased in some. Thus, it was evident that they are not highly correlated. However, there are some features with high correlation values, making them promising candidates for modeling.

## Modeling
In the problem definition phase, it was established that the ultimate objective of this project is to forecast sales. To achieve this goal, we need to develop a model that captures the relationship between different features and the target variable. In time series modeling, future values are often dependent on past values of the time series signal. In the case of multivariate time series, these relationships can exist among different time series and at various time lags, adding complexity to the analysis.

I categorize time series models into classic and modern methods. Classic methods involve techniques based on AutoRegressive (AR) and Moving Average (MA) models, which are adapted to handle different time series characteristics such as stationarity or seasonality. These models, originally designed for univariate time series analysis, can be extended to multivariate time series by considering the relationships between these signals. Given that our dataset is a multivariate time series with two time series and additional exogenous variables providing supplementary information for our model, I found VARMAX to be a suitable candidate. VARMAX can effectively manage the seasonal behavior of the signals.

Modern methods encompass machine learning and deep learning techniques. These approaches can handle more intricate relationships between features, making them particularly useful in scenarios with numerous features and large datasets. For a machine learning-based approach, I selected the XGBoost model due to its outstanding performance in both regression and classification tasks. Based on the analysis of the seasonal component of the decomposed signal, I determined the sequence length to be 7.

Finally, I explored deep learning models for this problem. The Recurrent Neural Network (RNN) family, particularly Long Short-Term Memory (LSTM), is a compelling choice for time series forecasting problems. I leveraged all 22 features and constructed the data sequences with a lag of 7 steps. For classical methods, I utilized the Python statsmodels package, which offers a comprehensive suite of algorithms. Additionally, alternative Python packages such as Prophet from the Facebook team can be considered. Prophet is known for its ease of use and automatic handling of many modeling challenges.
In deep learning modeling, I employed PyTorch, one of the premier deep learning libraries. Alternatively, tools like TensorFlow or Keras could also be utilized for similar purposes.
### Modeling Result
Classical model
To assess whether incorporating multivariate time series with exogenous variables enhances model accuracy, I initially began with the basic ARIMA model solely for the "vkmge" variable. Initially, I employed a univariate analysis, transitioning to a SARIMAX model to introduce seasonal order. While the inclusion of seasonal order added seasonal treatment to the model, the performance error remained unsatisfactory. Subsequently, I integrated the exogenous variables to assess any improvements in accuracy. However, the addition of exogenous variables led to the forecasting of negative values, prompting a systematic investigation into the features contributing to this issue. Notably, the "am" feature exerted a significant influence, albeit its removal did not entirely rectify the problem. Incrementally adding binary features, I discovered that the binary flag for "A" and "N" was the primary cause of predicting negative values. Incorporating seasonal orders into univariate models induced fluctuations in predicted values but failed to enhance accuracy. Upon integrating exogenous variables into the ARIMA model, an anticipated improvement was observed. Subsequently, the second time series "a" was included as a feature, leading to the creation of a VARMAX model as the suitable multivariate time series model. Comparative evaluation of metrics revealed improvements in "RMSE" and "MAPE". While the final model's forecasts on the evaluation data remained subpar, the exercise demonstrated that incorporating exogenous variables and a second time series could augment accuracy to some extent.

ML model
The XGBoost model trained with a 7-step lag outperformed classical models; however, it struggled with handling high values of "vkmge". Without outlier rejection, determining whether these high values were outliers or within the normal range remained uncertain. While tuning hyperparameters might improve tracking of target values, it risks overfitting the model. Additionally, creating lagged features required using "a" features to prevent data leakage, as lagged "vkmge" ground truth values might not be available for the forecasting period.

DL Model
Initially, I constructed an LSTM model comprising a single LSTM layer with 64 hidden nodes, followed by a fully connected layer. Dropout regularization was introduced, and "vkmge" and "a" timeseries values were normalized. A sequence of length 7 was created, excluding "vkmge" from the input sequence to prevent data leakage. However, evaluation on the test dataset revealed the model's inability to capture fluctuations in the target feature, indicating underfitting. To address this, I augmented the model complexity by adding three additional LSTM layers, but this adjustment did not yield performance improvements.

## Recap 
The training dataset comprises collected data, likely concerning the quantity of sales of an article in a store. The objective was to forecast the sales quantity based on other features. One feature represented a time series signal, while the others were binary and categorical values considered as exogenous variables, enhancing model accuracy. A major challenge was the significant amount of missing values, approximately 44% of the dataset. Given the absence of information about these features and their simultaneous missingness, it was assumed they were collected from a common resource unavailable during that period. Instead of imputing values, a new binary flag was defined to indicate missingness.

Exploratory data analysis (EDA) revealed another challenge: a period of approximately 3 to 4 months where the store was closed based on the "lt" flag, yet sales remained at zero after reopening. Due to insufficient information, it was decided to retain these entries rather than remove or impute them. Stationarity and causality tests were conducted to gain further insight into the dataset.

Three different methods were employed for time series forecasting. While none performed satisfactorily, the machine learning-based approach (XGBoost) showed promise. Deep learning models struggled to capture fluctuations in the target values. Although classical methods (VARMAX) did not exhibit worse performance than XGBoost based on evaluation metrics, they predicted some negative values, leading to their exclusion. Consequently, XGBoost was chosen for predicting the test dataset. I have to add that I did not do any action against outliers. Because of the lack of information about the data set I could not make decision about what should I do against the high value sales. So I preferred to keep them.

In conclusion, obtaining additional information about the data could facilitate more meaningful feature engineering and imputation of missing values, potentially enhancing model performance.